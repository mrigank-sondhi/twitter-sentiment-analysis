{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a64c38",
      "metadata": {
        "id": "c7a64c38"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import string\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "490c5d28",
      "metadata": {
        "id": "490c5d28"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50639681",
      "metadata": {
        "id": "50639681"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('training_twitter_x_y_train.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ff5e3e",
      "metadata": {
        "id": "86ff5e3e"
      },
      "outputs": [],
      "source": [
        "X_train = train_data['text']\n",
        "Y_train = train_data['airline_sentiment'] "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7cfa29a",
      "metadata": {
        "id": "e7cfa29a",
        "outputId": "22e36ba9-8d82-4670-e14c-e737b383c0e3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1637: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_single_block(indexer, value, name)\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(Y_train)):\n",
        "    if(Y_train.iloc[i] == 'positive'):\n",
        "        Y_train.iloc[i] = 0\n",
        "    elif(Y_train.iloc[i] == 'negative'):\n",
        "        Y_train.iloc[i] = 1\n",
        "    else:\n",
        "        Y_train.iloc[i] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce517006",
      "metadata": {
        "id": "ce517006"
      },
      "outputs": [],
      "source": [
        "classes = ['positive', 'negative', 'neutral']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2464c53",
      "metadata": {
        "id": "e2464c53"
      },
      "outputs": [],
      "source": [
        "def get_simple_pos_tag(nltk_pos_tag):\n",
        "    if nltk_pos_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_pos_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_pos_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ee7f1bb",
      "metadata": {
        "id": "5ee7f1bb"
      },
      "outputs": [],
      "source": [
        "#function to preprocess the words list to remove punctuations\n",
        "def preprocess(words_list):\n",
        "    #we create an empty translation table where every character in the first argument is mapped to\n",
        "    #every character in the second argument and every character in the third argument is mapped to\n",
        "    #none  \n",
        "    \n",
        "    #\" \\t \" in a word becomes none\n",
        "    translation_table = str.maketrans('', '', '\\t')\n",
        "    words_list = [word.translate(translation_table) for word in words_list]\n",
        "    \n",
        "    #\" ' \" appears in a lot of words and would change the meaning of the words if removed,\n",
        "    #hence it is removed from the list of punctuations we plan to remove from the words\n",
        "    punctuations = (string.punctuation).replace(\"'\", \"\") \n",
        "    #all punctuation characters become none \n",
        "    translation_table = str.maketrans('', '', punctuations)\n",
        "    words_list = [word.translate(translation_table) for word in words_list]\n",
        "    \n",
        "    #removing blank strings\n",
        "    words_list = [word for word in words_list if word]\n",
        "    \n",
        "    #some words are quoted in the documents and as we have not removed \" ' \" to maintain \n",
        "    #the meaning of the words, we try to unquote such words below\n",
        "    for i in range(len(words_list)):\n",
        "        if ((words_list[i][0] == \"'\") and (words_list[i][-1] == \"'\")):\n",
        "            words_list[i] = words_list[i][1:-1]\n",
        "        elif(words_list[i][0] == \"'\"):\n",
        "            words_list[i] = words_list[i][1:]\n",
        "        \n",
        "    #we will also remove just numeric strings as they do not have any significant meaning in \n",
        "    #text classification\n",
        "    words_list = [word for word in words_list if not word.isdigit()]\n",
        "    \n",
        "    #removing blank strings\n",
        "    words_list = [word for word in words_list if word]\n",
        "    \n",
        "    #making all words lower-case\n",
        "    #words_list = [word.lower() for word in words_list]\n",
        "    \n",
        "    #removing words with two or less characters\n",
        "    words_list = [word for word in words_list if (len(word) > 2)]\n",
        "    \n",
        "    return words_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5767282",
      "metadata": {
        "id": "f5767282"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "punctuation = list(string.punctuation)\n",
        "stop_words.update(punctuation)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() \n",
        "\n",
        "def clean_review(word_list):\n",
        "    word_list = preprocess(word_list)\n",
        "    final_words = []\n",
        "    for word in word_list:\n",
        "        if word.lower() not in stop_words:\n",
        "            pos = pos_tag([word])\n",
        "            clean_word = lemmatizer.lemmatize(word, pos = get_simple_pos_tag(pos[0][1]))\n",
        "            final_words.append(clean_word.lower())\n",
        "    return final_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b49772",
      "metadata": {
        "id": "d6b49772"
      },
      "outputs": [],
      "source": [
        "training_tweets = []\n",
        "\n",
        "for tweet in X_train:\n",
        "    final_tweet = \" \".join(clean_review(tweet.split(' ')))\n",
        "    training_tweets.append(final_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8281a040",
      "metadata": {
        "id": "8281a040"
      },
      "outputs": [],
      "source": [
        "# count_vectorizer = CountVectorizer(max_features = 5000, max_df = 1, min_df = 1, ngram_range = (1, 1))\n",
        "# X_train_transformed = count_vectorizer.fit_transform(training_tweets)\n",
        "# Y_train = np.asarray(Y_train, dtype = 'int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4376779b",
      "metadata": {
        "id": "4376779b"
      },
      "outputs": [],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_features = 5000, max_df = 1, min_df = 1, ngram_range = (1, 1))\n",
        "X_train_transformed = tfidf_vectorizer.fit_transform(training_tweets)\n",
        "Y_train = np.asarray(Y_train, dtype = 'int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27d11fed",
      "metadata": {
        "id": "27d11fed",
        "outputId": "412c36f3-d73a-40d2-fe0b-0ebbcafcb98c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7410746812386156"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "svc = SVC()\n",
        "svc.fit(X_train_transformed, Y_train)\n",
        "svc.score(X_train_transformed, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75c7621d",
      "metadata": {
        "id": "75c7621d",
        "outputId": "ac8f4cb0-7e7b-442d-b19c-fa838f29972b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6467213114754098"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_transformed, Y_train)\n",
        "mnb.score(X_train_transformed, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06fc3a17",
      "metadata": {
        "id": "06fc3a17",
        "outputId": "c8874ea9-1a74-486d-d427-81c7c01a3464"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7410746812386156"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X_train_transformed, Y_train)\n",
        "dt.score(X_train_transformed, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4481ffe",
      "metadata": {
        "id": "e4481ffe"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b161965a",
      "metadata": {
        "id": "b161965a"
      },
      "outputs": [],
      "source": [
        "test_data = pd.read_csv('test_twitter_x_test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1d5f4a",
      "metadata": {
        "id": "6c1d5f4a"
      },
      "outputs": [],
      "source": [
        "X_test = test_data['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e9df470",
      "metadata": {
        "id": "2e9df470"
      },
      "outputs": [],
      "source": [
        "testing_tweets = []\n",
        "\n",
        "for tweet in X_test:\n",
        "    final_tweet = \" \".join(clean_review(tweet.split(' ')))\n",
        "    testing_tweets.append(final_tweet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdda3876",
      "metadata": {
        "id": "cdda3876"
      },
      "outputs": [],
      "source": [
        "X_test_transformed = tfidf_vectorizer.transform(testing_tweets)\n",
        "Y_predictions = svc.predict(X_test_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ed2bc5",
      "metadata": {
        "id": "c2ed2bc5"
      },
      "outputs": [],
      "source": [
        "data_type = np.dtype('U25')\n",
        "result = np.empty((len(Y_predictions)), dtype = data_type) \n",
        "for i in range(len(Y_predictions)):\n",
        "    result[i] = classes[Y_predictions[i]]\n",
        "np.savetxt(\"submission.csv\", result, fmt = '%s')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "TwitterSentimentAnalysis.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}